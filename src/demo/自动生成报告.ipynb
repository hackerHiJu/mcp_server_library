{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "OPEN_ROUTER = \"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\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"你是一个报告生成助手。\n",
    "你可以使用 MCP 服务器提供的工具来完成任务。\n",
    "MCP 服务器会动态提供工具，你需要先检查当前可用的工具。\n",
    "\n",
    "在使用 MCP 工具时，请遵循以下步骤：\n",
    "1、根据任务需求选择合适的工具\n",
    "2、按照工具的参数要求提供正确的参数\n",
    "3、观察工具的返回结果，并根据结果决定下一步操作\n",
    "4、工具可能会发生变化，比如新增工具或现有工具消失\n",
    "\n",
    "请遵循以下指南：\n",
    "- 使用工具时，确保参数符合工具的文档要求\n",
    "- 如果出现错误，请理解错误原因并尝试用修正后的参数重新调用\n",
    "- 按照任务需求逐步完成，优先选择最合适的工具\n",
    "- 如果需要连续调用多个工具，请一次只调用一个工具并等待结果\n",
    "- 以```json```格式输出。例如：```json{\"name\": \"tool_name\", \"params\": {\"param1\": \"value1\", \"param2\": \"value2\"}}```\n",
    "\n",
    "请清楚地向用户解释你的推理过程和操作步骤。\n",
    "\n",
    "可选择的工具如下：\n",
    "\"\"\"\n",
    "\n",
    "NEXT_STEP_PROMPT = \"\"\"\n",
    "## 任务目标\n",
    "根据已经获取的信息，判断是否可以解决用户的需求。\n",
    "\n",
    "## 任务要求\n",
    "- 请认真审视用户的需求，特别注意用户需求中的条件和范围\n",
    "- 如果可以解决(满足用户给出的条件和范围)，请输出<finish>\n",
    "- 如果缺少数据或内容，请继续调用合适的工具获取更多信息\n",
    "\n",
    "## 用户需求\n",
    "用户需求如下:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "FINISH_GENETATE = '''\n",
    "## 任务目标\n",
    "根据已收集信息和用户需求生成完整报告。\n",
    "\n",
    "## 已收集信息\n",
    "{}\n",
    "\n",
    "## 任务要求\n",
    "1、请根据图片的描述将图片链接插入到合适的位置，如果没有符合要求的图片，请不要插入图片\n",
    "2、以markdown格式生成报告\n",
    "\n",
    "## 用户需求\n",
    "{}'''"
   ],
   "id": "e3634c6edd580aa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from mcp.server.fastmcp import FastMCP\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "mcp = FastMCP(\"search\")\n",
    "\n",
    "base_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = 'aaa'\n",
    "model_name = 'deepseek/deepseek-chat:free'\n",
    "\n",
    "# 创建日志记录器\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 创建控制台处理器\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(console_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# 创建文件处理器\n",
    "file_handler = logging.FileHandler('test.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_query(query, stream=False):\n",
    "    prompt = \"\"\"You are an expert research assistant. Given the user's query, generate up to four distinct, precise search queries that would help gather comprehensive information on the topic.\n",
    "    Return only a Python list of strings, for example: ['query1', 'query2', 'query3'].\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful and precise research assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"User Query: {query}\\n\\n{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def if_useful(query: str, page_text: str):\n",
    "    prompt = \"\"\"You are a critical research evaluator. Given the user's query and the content of a webpage, determine if the webpage contains information relevant and useful for addressing the query.\n",
    "    Respond with exactly one word: 'Yes' if the page is useful, or 'No' if it is not. Do not include any extra text.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a strict and concise evaluator of research relevance.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"User Query: {query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].message.content\n",
    "\n",
    "    if response:\n",
    "        answer = response.strip()\n",
    "        if answer in [\"Yes\", \"No\"]:\n",
    "            return answer\n",
    "        else:\n",
    "            # Fallback: try to extract Yes/No from the response.\n",
    "            if \"Yes\" in answer:\n",
    "                return \"Yes\"\n",
    "            elif \"No\" in answer:\n",
    "                return \"No\"\n",
    "    return \"No\"\n",
    "\n",
    "\n",
    "def extract_relevant_context(query, search_query, page_text):\n",
    "    prompt = \"\"\"You are an expert information extractor. Given the user's query, the search query that led to this page, and the webpage content, extract all pieces of information that are relevant to answering the user's query.\n",
    "    Return only the relevant context as plain text without commentary.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in extracting and summarizing relevant information.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"User Query: {query}\\nSearch Query: {search_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].message.content\n",
    "    if response:\n",
    "        return response.strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_new_search_queries(user_query, previous_search_queries, all_contexts):\n",
    "    context_combined = \"\\n\".join(all_contexts)\n",
    "    prompt = \"\"\"You are an analytical research assistant. Based on the original query, the search queries performed so far, and the extracted contexts from webpages, determine if further research is needed.\n",
    "    If further research is needed, provide up to four new search queries as a Python list (for example, ['new query1', 'new query2']). If you believe no further research is needed, respond with exactly .\n",
    "    Output only a Python list or the token  without any additional text.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in extracting and summarizing relevant information.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"User Query: {user_query}\\nPrevious Search Queries: {previous_search_queries}\\n\\nExtracted Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].message.content\n",
    "    if response:\n",
    "        cleaned = response.strip()\n",
    "        if cleaned == \"\":\n",
    "            return \"\"\n",
    "        try:\n",
    "            new_queries = eval(cleaned)\n",
    "            if isinstance(new_queries, list):\n",
    "                return new_queries\n",
    "            else:\n",
    "                logger.info(f\"LLM did not return a list for new search queries. Response: {response}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing new search queries:{e}, Response:{response}\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def web_search(query: str, ) -> str:\n",
    "    links = []\n",
    "    response = requests.get(\n",
    "        f'http://10.250.2.24:8088/search?format=json&q={query}&language=zh-CN&time_range=&safesearch=0&categories=general', timeout=10)\n",
    "    results = response.json()['results']\n",
    "    for result in results[:2]:\n",
    "        links.append(result['url'])\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def fetch_webpage_text(url):\n",
    "    JINA_BASE_URL = 'https://r.jina.ai/'\n",
    "    full_url = f\"{JINA_BASE_URL}{url}\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(full_url, timeout=50)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.text\n",
    "        else:\n",
    "            text = resp.text\n",
    "            logger.info(f\"Jina fetch error for {url}: {resp.status_code} - {text}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching webpage text with Jina:{e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_link(link, query, search_query):\n",
    "    logger.info(f\"Fetching content from: {link}\")\n",
    "    page_text = fetch_webpage_text(link)\n",
    "    if not page_text:\n",
    "        return None\n",
    "    usefulness = if_useful(query, page_text)\n",
    "    logger.info(f\"Page usefulness for {link}: {usefulness}\")\n",
    "    if usefulness == \"Yes\":\n",
    "        context = extract_relevant_context(query, search_query, page_text)\n",
    "        if context:\n",
    "            logger.info(f\"Extracted context from {link} (first 200 chars): {context[:200]}\")\n",
    "            return context\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_images_description(iamge_url):\n",
    "    completion = client.chat.completions.create(\n",
    "\n",
    "        model=\"qwen/qwen2.5-vl-32b-instruct:free\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"使用一句话描述图片的内容\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": iamge_url\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"互联网搜索\"\"\"\n",
    "    iteration_limit = 3\n",
    "    iteration = 0\n",
    "    aggregated_contexts = []\n",
    "    all_search_queries = []\n",
    "    iteration = 0\n",
    "\n",
    "    new_search_queries = eval(generate_query(query))\n",
    "    all_search_queries.extend(new_search_queries)\n",
    "    while iteration < iteration_limit:\n",
    "        logger.info(f\"\\n=== Iteration {iteration + 1} ===\")\n",
    "        iteration_contexts = []\n",
    "        search_results = [web_search(query) for query in new_search_queries]\n",
    "\n",
    "        unique_links = {}\n",
    "        for idx, links in enumerate(search_results):\n",
    "            query = new_search_queries[idx]\n",
    "            for link in links:\n",
    "                if link not in unique_links:\n",
    "                    unique_links[link] = query\n",
    "\n",
    "        logger.info(f\"Aggregated {len(unique_links)} unique links from this iteration.\")\n",
    "\n",
    "        # Process each link concurrently: fetch, judge, and extract context.\n",
    "        link_results = [\n",
    "            process_link(link, query, unique_links[link])\n",
    "            for link in unique_links\n",
    "        ]\n",
    "\n",
    "        # Collect non-None contexts.\n",
    "        for res in link_results:\n",
    "            if res:\n",
    "                iteration_contexts.append(res)\n",
    "\n",
    "        if iteration_contexts:\n",
    "            aggregated_contexts.extend(iteration_contexts)\n",
    "        else:\n",
    "            logger.info(\"No useful contexts were found in this iteration.\")\n",
    "\n",
    "        new_search_queries = get_new_search_queries(query, all_search_queries, aggregated_contexts)\n",
    "        if new_search_queries == \"\":\n",
    "            logger.info(\"LLM indicated that no further research is needed.\")\n",
    "            break\n",
    "        elif new_search_queries:\n",
    "            logger.info(f\"LLM provided new search queries:{new_search_queries}\")\n",
    "            all_search_queries.extend(new_search_queries)\n",
    "        else:\n",
    "            logger.info(\"LLM did not provide any new search queries. Ending the loop.\")\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "    return '\\n\\n'.join(aggregated_contexts)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_images(query: str) -> str:\n",
    "    '''获取图片链接和描述'''\n",
    "    logger.info(f\"Searching for images for query: {query}\")\n",
    "    response = requests.get(\n",
    "        f'http://10.250.2.24:8088/search?format=json&q={query}&language=zh-CN&time_range=month&safesearch=0&categories=images')\n",
    "    results = response.json()['results']\n",
    "    img_srcs = []\n",
    "    for result in results[:2]:\n",
    "        img_srcs.append(result['img_src'])\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for img_src in img_srcs:\n",
    "        logger.info(f\"Fetching image description for: {img_src}\")\n",
    "        description = get_images_description(img_src)\n",
    "        logger.info(f\"Image description for {img_src}: {description}\")\n",
    "        result[img_src] = description\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ],
   "id": "9681d6171d0ab13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import Optional\n",
    "from openai import AsyncOpenAI\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "base_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = 'aaa'\n",
    "model_name = 'deepseek/deepseek-chat:free'\n",
    "\n",
    "\n",
    "def get_clear_json(text):\n",
    "    if '```json' not in text:\n",
    "        return 0, text\n",
    "\n",
    "    return 1, text.split('```json')[1].split('```')[0]\n",
    "\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.client = AsyncOpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "\n",
    "    async def connect_to_server(self, server_script_path: str):\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"python\",\n",
    "            args=[server_script_path],\n",
    "            env=None\n",
    "        )\n",
    "\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "        self.stdio, self.write = stdio_transport\n",
    "        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "\n",
    "        await self.session.initialize()\n",
    "\n",
    "        # 列出可用工具\n",
    "        response = await self.session.list_tools()\n",
    "        tools = response.tools\n",
    "        logger.info(f\"\\nConnected to server with tools: {[tool.name for tool in tools]}\")\n",
    "\n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"使用 LLM 和 MCP 服务器提供的工具处理查询\"\"\"\n",
    "\n",
    "        response = await self.session.list_tools()\n",
    "\n",
    "        available_tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema\n",
    "            }\n",
    "        } for tool in response.tools]\n",
    "        logger.info(f'available_tools:\\n\\n{available_tools}')\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT + str(available_tools)\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        message = response.choices[0].message\n",
    "        logger.info(f'llm_output(tool call)：{message.content}')\n",
    "\n",
    "        results = []\n",
    "        while True:\n",
    "\n",
    "            flag, json_text = get_clear_json(message.content)\n",
    "\n",
    "            if flag == 0:\n",
    "                response = await self.client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": query}]\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "\n",
    "            json_text = json.loads(json_text)\n",
    "            tool_name = json_text['name']\n",
    "            tool_args = json_text['params']\n",
    "            result = await self.session.call_tool(tool_name, tool_args)\n",
    "            logger.info(f'tool name: \\n{tool_name}\\ntool call result: \\n{result}')\n",
    "            results.append(result.content[0].text)\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": message.content\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'工具调用结果如下：{result}'\n",
    "            })\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": NEXT_STEP_PROMPT.format(query)\n",
    "            })\n",
    "\n",
    "            response = await self.client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "            logger.info(f'llm_output：\\n{message.content}')\n",
    "\n",
    "            if 'finish' in message.content:\n",
    "                break\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": message.content\n",
    "            })\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": FINISH_GENETATE.format('\\n\\n'.join(results), query)\n",
    "        })\n",
    "\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        message = response.choices[0].message.content\n",
    "        return message\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"运行交互式聊天循环\"\"\"\n",
    "        logger.info(\"\\nMCP Client Started!\")\n",
    "        logger.info(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                response = await self.process_query(query)\n",
    "                print(response)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"\\nError: {str(e)}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    client = MCPClient()\n",
    "    await client.connect_to_server('./search_mcp.py')\n",
    "    await client.chat_loop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ],
   "id": "84c82828af993f4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
